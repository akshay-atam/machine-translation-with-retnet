{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-fr')\n",
    "\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232825/232825 [00:00<00:00, 4175455.49it/s]\n",
      "100%|██████████| 232825/232825 [00:00<00:00, 4155254.51it/s]\n",
      "100%|██████████| 890/890 [00:00<00:00, 252891.44it/s]\n",
      "100%|██████████| 890/890 [00:00<00:00, 125831.95it/s]\n",
      "100%|██████████| 8597/8597 [00:00<00:00, 1562256.03it/s]\n",
      "100%|██████████| 8597/8597 [00:00<00:00, 32780392.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_en_fr_sentences(data):\n",
    "    english = [item['en'] for item in tqdm(data['translation'])]\n",
    "    french = [item['fr'] for item in tqdm(data['translation'])]\n",
    "\n",
    "    return english, french\n",
    "\n",
    "train_en, train_fr = get_en_fr_sentences(train_data)\n",
    "val_en, val_fr = get_en_fr_sentences(val_data)\n",
    "test_en, test_fr = get_en_fr_sentences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 random indices and select English and French sentence pairs\n",
    "import random\n",
    "random.seed(627)\n",
    "\n",
    "num_sentences = 100\n",
    "random_indices = random.sample(range(len(train_en)), num_sentences)\n",
    "\n",
    "# get English and French sentences\n",
    "# English: 'sentence_eng_Latn' \n",
    "# French: 'sentence_fra_Latn' \n",
    "english_sentences = [train_en[i] for i in random_indices]\n",
    "french_sentences = [train_fr[i] for i in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MarianMTModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MarianMTModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "# store generated sentences of OPUS-MT\n",
    "opus_mt_sentences = []\n",
    "\n",
    "# loop through each english sentence\n",
    "for sentence in english_sentences:\n",
    "    tokenizer.src_lang = \"en\" \n",
    "    encoded_en = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_en)\n",
    "    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    opus_mt_sentences.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# store generated sentences of M2M-100\n",
    "m2m_100_sentences = []\n",
    "\n",
    "# loop through each english sentence\n",
    "for sentence in english_sentences:\n",
    "    tokenizer.src_lang = \"en\"\n",
    "    encoded_en = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n",
    "    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    m2m_100_sentences.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# store generated sentences from MBART-50\n",
    "mbart_50_sentences = []\n",
    "\n",
    "# loop through each english sentence\n",
    "for sentence in english_sentences:\n",
    "    \n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    encoded_en = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\n",
    "    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    mbart_50_sentences.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Minimum sentence length: 4 \n",
      "Average sentence length: 20.02 \n",
      "Maximum sentence length: 60\n",
      "==================================================\n",
      "French\n",
      "French - Minimum sentence length: 2 \n",
      "Average sentence length: 20.81 \n",
      "Maximum sentence length: 58\n"
     ]
    }
   ],
   "source": [
    "# using OPUS-MT and AutoTokenizer as it had highest BLEU score\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "tokenized_inputs = tokenizer(english_sentences, french_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the input IDs\n",
    "input_ids = tokenized_inputs['input_ids']\n",
    "\n",
    "# compute english and french sentence lengths\n",
    "english_sentence_lengths = [len(sentence.split()) for sentence in english_sentences]\n",
    "french_sentence_lengths = [len(sentence.split()) for sentence in french_sentences]\n",
    "\n",
    "# basic min, max and avg\n",
    "min_length_english = min(english_sentence_lengths)\n",
    "avg_length_english = sum(english_sentence_lengths) / len(english_sentence_lengths)\n",
    "max_length_english = max(english_sentence_lengths)\n",
    "\n",
    "min_length_french = min(french_sentence_lengths)\n",
    "avg_length_french = sum(french_sentence_lengths) / len(french_sentence_lengths)\n",
    "max_length_french = max(french_sentence_lengths)\n",
    "\n",
    "print(\"English\")\n",
    "print(f\"Minimum sentence length: {min_length_english} \\nAverage sentence length: {avg_length_english} \\nMaximum sentence length: {max_length_english}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"French\")\n",
    "print(f\"French - Minimum sentence length: {min_length_french} \\nAverage sentence length: {avg_length_french} \\nMaximum sentence length: {max_length_french}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for OPUS-MT: 31.859311215529004\n",
      "BLEU score for M2M-100: 26.592755777695388\n",
      "BLEU score for MBART-50: 29.52344400211202\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "models = [\"OPUS-MT\", \"M2M-100\", \"MBART-50\"]\n",
    "output_sentences = [opus_mt_sentences, m2m_100_sentences, mbart_50_sentences]\n",
    "\n",
    "for i, j in zip(models, output_sentences):\n",
    "    k = [str(sentence) for sentence in j]\n",
    "\n",
    "    # bleu score\n",
    "    bleu = sacrebleu.corpus_bleu(k, [french_sentences])\n",
    "\n",
    "    print(f\"BLEU score for {i}: {bleu.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "Ground Truth: Notre rêve est donc de rassembler les adolescents, pour qu'ils vivent une expérience collective d'entraide ainsi qu'une expérience interculturelle, en instruisant les enfants de ces régions et en les aidant à construire leurs moyens de communication.\n",
      "OPUS-MT output: [\"Notre rêve est donc de réunir les adolescents, afin qu'ils aient une expérience de service communautaire ainsi qu'une expérience interculturelle, car ils enseignent aux enfants dans ces domaines et les aident à construire leur infrastructure de communication.\"]\n",
      "M2M-100 output: ['Ainsi, notre rêve est de rassembler les adolescents, de sorte qu’ils auront une expérience de service communautaire ainsi qu’une expérience interculturelle, car ils enseignent aux enfants dans ces domaines et les aident à construire leur infrastructure de communication.']\n",
      "MBART-50 output: [\"Notre rêve est donc de rassembler les adolescents afin qu'ils aient une expérience de services communautaires ainsi qu'une expérience interculturelle, alors qu'ils enseignent aux enfants dans ces régions et les aident à bâtir leur infrastructure de communication.\"]\n",
      "==================================================\n",
      "Sentence 2\n",
      "Ground Truth: Elles passent la majeure partie de leurs vies, jusqu'à 2 ans, sous cette forme larvaire.\n",
      "OPUS-MT output: [\"Ils passent la grande majorité de leur vie, jusqu'à deux ans, dans cette forme larvaire.\"]\n",
      "M2M-100 output: [\"Ils passent la grande majorité de leur vie, jusqu'à deux ans, dans cette forme larve.\"]\n",
      "MBART-50 output: [\"Ils passent la grande majorité de leur vie, jusqu'à deux ans, sous cette forme larvaire.\"]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "idx = [6, 27]\n",
    "\n",
    "for k, v in enumerate(idx):\n",
    "    print(f\"Sentence {k+1}\")\n",
    "    print(f\"Ground Truth: {french_sentences[v]}\")\n",
    "    print(f\"OPUS-MT output: {opus_mt_sentences[v]}\")\n",
    "    print(f\"M2M-100 output: {m2m_100_sentences[v]}\")\n",
    "    print(f\"MBART-50 output: {mbart_50_sentences[v]}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
